{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqQasATtgKvY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d818030e-d2b1-4be4-e7c1-927386add4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: jupyter-nbconvert [-h] [--debug] [--show-config] [--show-config-json]\n",
            "                         [--generate-config] [-y] [--execute] [--allow-errors]\n",
            "                         [--stdin] [--stdout] [--inplace] [--clear-output]\n",
            "                         [--coalesce-streams] [--no-prompt] [--no-input]\n",
            "                         [--allow-chromium-download]\n",
            "                         [--disable-chromium-sandbox] [--show-input]\n",
            "                         [--embed-images] [--sanitize-html]\n",
            "                         [--log-level NbConvertApp.log_level]\n",
            "                         [--config NbConvertApp.config_file]\n",
            "                         [--to NbConvertApp.export_format]\n",
            "                         [--template TemplateExporter.template_name]\n",
            "                         [--template-file TemplateExporter.template_file]\n",
            "                         [--theme HTMLExporter.theme]\n",
            "                         [--sanitize_html HTMLExporter.sanitize_html]\n",
            "                         [--writer NbConvertApp.writer_class]\n",
            "                         [--post NbConvertApp.postprocessor_class]\n",
            "                         [--output NbConvertApp.output_base]\n",
            "                         [--output-dir FilesWriter.build_directory]\n",
            "                         [--reveal-prefix SlidesExporter.reveal_url_prefix]\n",
            "                         [--nbformat NotebookExporter.nbformat_version]\n",
            "                         [extra_args ...]\n",
            "jupyter-nbconvert: error: unrecognized arguments: EduAssist.ipynb\n"
          ]
        }
      ],
      "source": [
        "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True \\\n",
        "    --to notebook --output fixed_notebook.ipynb \\\n",
        "    !jupyter nbconvert --ClearMetadataPreprocessor.enabled=True \\\n",
        "    --to notebook --output fixed_notebook.ipynb \\\n",
        "    \"EduAssist.ipynb\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "# !pip install --upgrade transformers\n",
        "# !pip install emoji"
      ],
      "metadata": {
        "id": "4jPkf4YtgeIB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pdfplumber PyPDF2"
      ],
      "metadata": {
        "id": "WojfKqqGg1uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install --quiet  nltk faiss-cpu"
      ],
      "metadata": {
        "id": "JctshoZahAtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install torch\n",
        "# !pip install emoji\n",
        "# !pip install nltk\n",
        "# !pip install pdfplumber\n",
        "# !pip install PyPDF2\n",
        "# !pip install sentence-transformers\n",
        "# !pip install faiss-cpu\n",
        "# !pip install python-telegram-bot==20.0\n",
        "# !pip install nest_asyncio"
      ],
      "metadata": {
        "id": "-Qq7KfvihFfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nest_asyncio\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "TOKEN = \"8326794111:AAHWrN70roC7EK9rJz9LJyjS17ugMNcx5dM\"\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
        "import torch, re, emoji, pdfplumber, io\n",
        "from PyPDF2 import PdfReader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import asyncio\n",
        "\n",
        "from telegram import ReplyKeyboardMarkup, Update\n",
        "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
        "from typing import Tuple, List, Dict"
      ],
      "metadata": {
        "id": "QEFsNRw5hbrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rag"
      ],
      "metadata": {
        "id": "Ahtfka9B8kpv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGStore:\n",
        "    def __init__(self):\n",
        "        self.chunks: List[str] = []\n",
        "        self.metas: List[Dict] = []\n",
        "        self.index = None\n",
        "        self.emb_dim = None\n",
        "\n",
        "    def add_text(self, text: str, source: str = \"user\") -> None:\n",
        "        \"\"\"Ø¥Ø¶Ø§ÙØ© Ù†Øµ Ø¬Ø¯ÙŠØ¯ Ø¥Ù„Ù‰ Ù…Ø®Ø²Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ©\"\"\"\n",
        "        chunks = self._chunk_text(text)\n",
        "        embs = embedder.encode(chunks, convert_to_numpy=True, normalize_embeddings=True)\n",
        "\n",
        "        if self.index is None:\n",
        "            self.emb_dim = embs.shape[1]\n",
        "            self.index = faiss.IndexFlatIP(self.emb_dim)\n",
        "        self.index.add(embs)\n",
        "\n",
        "        start_id = len(self.chunks)\n",
        "        self.chunks.extend(chunks)\n",
        "        for i in range(len(chunks)):\n",
        "            self.metas.append({\"source\": source, \"chunk_id\": start_id + i})\n",
        "\n",
        "    @staticmethod\n",
        "    def _chunk_text(text: str, chunk_words: int = 400) -> List[str]:\n",
        "        \"\"\"ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£ØµØºØ±\"\"\"\n",
        "        words = text.split()\n",
        "        return [' '.join(words[i:i+chunk_words]) for i in range(0, len(words), chunk_words)]\n",
        "\n",
        "    def search(self, query: str, top_k: int = 4) -> List[Tuple[str, Dict, float]]:\n",
        "        \"\"\"Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø°Ø§Øª Ø§Ù„ØµÙ„Ø© Ø¨Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨\"\"\"\n",
        "        if not self.chunks:\n",
        "            return []\n",
        "\n",
        "        q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True)\n",
        "        D, I = self.index.search(q_emb, min(top_k, len(self.chunks)))\n",
        "\n",
        "        return [\n",
        "            (self.chunks[idx], self.metas[idx], float(score))\n",
        "            for score, idx in zip(D[0], I[0])\n",
        "            if idx != -1\n",
        "        ]"
      ],
      "metadata": {
        "id": "Pg76wItwh1wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Loading"
      ],
      "metadata": {
        "id": "mngjccP081Ij"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "GEN_MODEL = \"google/flan-t5-large\"\n",
        "EMBED_MODEL = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(GEN_MODEL)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(GEN_MODEL).to(\"cuda\" if device == 0 else \"cpu\")\n",
        "\n",
        "text2text_pipe = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device,\n",
        "    max_length=512\n",
        ")\n",
        "\n",
        "embedder = SentenceTransformer(EMBED_MODEL, device=\"cuda\" if device == 0 else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {model.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m184Paq0iP5G",
        "outputId": "b5532de6-a463-41ef-ca5a-d73a2e53be7d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "ZEPUEiMr8-Nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text: str, remove_stopwords: bool = True) -> Tuple[str, int]:\n",
        "    \"\"\"Clean text from unwanted characters and emojis\"\"\"\n",
        "    emoji_count = sum(1 for c in text if c in emoji.EMOJI_DATA)\n",
        "    text = emoji.replace_emoji(text, replace=' ')\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s.,!?]', ' ', text)\n",
        "    lines = [line.strip() for line in text.splitlines() if len(line.strip()) > 5]\n",
        "    text = ' '.join(lines)\n",
        "\n",
        "    if remove_stopwords:\n",
        "        text = ' '.join(w for w in text.split() if w.lower() not in STOPWORDS)\n",
        "\n",
        "    return re.sub(r'\\s+', ' ', text).strip(), emoji_count\n",
        "\n",
        "async def process_with_typing_indicator(update: Update, context: ContextTypes.DEFAULT_TYPE, task, *args):\n",
        "    \"\"\"Helper function to show typing indicator during processing\"\"\"\n",
        "    await context.bot.send_chat_action(chat_id=update.effective_chat.id, action=\"typing\")\n",
        "    try:\n",
        "        return await task(*args)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processing: {e}\")\n",
        "        await update.message.reply_text(\"âŒ An error occurred during processing. Please try again.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "XUGiqBVFiVeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary Generation"
      ],
      "metadata": {
        "id": "0r6u55Vk9EI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_summary(text: str) -> str:\n",
        "    \"\"\"Generate a concise summary in English\"\"\"\n",
        "    chunks = RAGStore._chunk_text(text, 800)\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"Summarize the following text in 3-4 short bullet points. Focus only on the key facts. Remove unnecessary words and keep sentences clear:\\n\\n{chunk}\"\n",
        "\n",
        "\n",
        "        out = await asyncio.to_thread(\n",
        "            text2text_pipe,\n",
        "            prompt,\n",
        "            max_length=200,\n",
        "            min_length=60,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        if out and len(out) > 0:\n",
        "            summary = out[0]['generated_text'].strip()\n",
        "            if summary:\n",
        "                summaries.append(summary)\n",
        "\n",
        "    final = \"\\n\\n\".join(summaries)\n",
        "    return final if final.strip() else \"No summary could be generated.\""
      ],
      "metadata": {
        "id": "S4n0PKccjCSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simplified Explanation Generation"
      ],
      "metadata": {
        "id": "aneZwbuF9KK7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_simplified_explanation(text: str) -> str:\n",
        "    \"\"\"Generate a simplified explanation in plain English\"\"\"\n",
        "    chunks = RAGStore._chunk_text(text, 700)\n",
        "    explanations = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        prompt = f\"Explain this text in simple English for a high school student. Use short sentences and clear language:\\n\\n{chunk}\"\n",
        "        out = await asyncio.to_thread(\n",
        "            text2text_pipe,\n",
        "            prompt,\n",
        "            max_length=250,\n",
        "            min_length=80,\n",
        "            do_sample=True,\n",
        "            temperature=0.6,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "        if out and len(out) > 0:\n",
        "            explanation = out[0]['generated_text'].strip()\n",
        "            if explanation:\n",
        "                explanations.append(explanation)\n",
        "\n",
        "    final = \"\\n\\n\".join(explanations)\n",
        "    return final if final.strip() else \"No explanation could be generated.\""
      ],
      "metadata": {
        "id": "tyKvP6-VjFO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer Generation"
      ],
      "metadata": {
        "id": "m8XOcnhi9SPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async def generate_answer(question: str, contexts: List[str]) -> str:\n",
        "    \"\"\"Answer a question in English based on context\"\"\"\n",
        "    context_text = \"\\n\".join(contexts[:3])\n",
        "    prompt = f\"Answer the question based on the context below. If the answer is not in the context, say 'I don't know'.\\n\\nContext:\\n{context_text}\\n\\nQuestion: {question}\\nAnswer:\"\n",
        "\n",
        "    out = await asyncio.to_thread(\n",
        "        text2text_pipe,\n",
        "        prompt,\n",
        "        max_length=300,\n",
        "        do_sample=True,\n",
        "        temperature=0.5,\n",
        "        num_beams=3\n",
        "    )\n",
        "\n",
        "    if out and len(out) > 0:\n",
        "        answer = out[0]['generated_text'].strip()\n",
        "        return answer if answer else \"I don't know.\"\n",
        "    return \"I don't know.\""
      ],
      "metadata": {
        "id": "RpFq0131jIZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PDF Content Reader"
      ],
      "metadata": {
        "id": "ZS7y8pfo9Xur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_pdf_content(file_bytes: bytes) -> str:\n",
        "    \"\"\"Read PDF content using multiple libraries for reliability\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted = page.extract_text()\n",
        "                if extracted:\n",
        "                    text += extracted + \"\\n\"\n",
        "        if text.strip():\n",
        "            return text\n",
        "    except Exception as e:\n",
        "        print(f\"pdfplumber error: {e}\")\n",
        "\n",
        "    try:\n",
        "        reader = PdfReader(io.BytesIO(file_bytes))\n",
        "        for page in reader.pages:\n",
        "            extracted = page.extract_text()\n",
        "            if extracted:\n",
        "                text += extracted + \"\\n\"\n",
        "    except Exception as e:\n",
        "        print(f\"PyPDF2 error: {e}\")\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "N5XzPYbTjLCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bot"
      ],
      "metadata": {
        "id": "q8AxMDTr9b2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MENU_KEYBOARD = ReplyKeyboardMarkup(\n",
        "    [\n",
        "        [\"Summarize Text ðŸ“\", \"Explain Text ðŸ“–\"],\n",
        "        [\"Ask Questions â“\", \"Clear Memory ðŸ—‘ï¸\"],\n",
        "        [\"Help â„¹ï¸\"]\n",
        "    ],\n",
        "    resize_keyboard=True,\n",
        "    one_time_keyboard=False\n",
        ")\n",
        "\n",
        "user_data = {}\n",
        "\n",
        "async def start_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_id = update.message.from_user.id\n",
        "    user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
        "    await update.message.reply_text(\n",
        "        \"ðŸ‘‹ Welcome to the Study Assistant Bot!\\n\\n\"\n",
        "        \"I can help you with:\\n\"\n",
        "        \"â€¢ ðŸ“ Summarizing texts and PDFs\\n\"\n",
        "        \"â€¢ ðŸ“– Explaining content in simple English\\n\"\n",
        "        \"â€¢ â“ Answering your questions about the material\\n\\n\"\n",
        "        \"Send me a text or PDF to get started!\",\n",
        "        reply_markup=MENU_KEYBOARD\n",
        "    )\n",
        "\n",
        "async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    await update.message.reply_text(\n",
        "        \"ðŸ†˜ Help:\\n\\n\"\n",
        "        \"â€¢ Send a PDF or text\\n\"\n",
        "        \"â€¢ Use the menu to Summarize, Explain, or Ask\\n\"\n",
        "        \"â€¢ Type 'exit' to leave question mode\",\n",
        "        reply_markup=MENU_KEYBOARD\n",
        "    )\n",
        "\n",
        "async def handle_menu_selection(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_id = update.message.from_user.id\n",
        "    if user_id not in user_data:\n",
        "        await start_command(update, context)\n",
        "        return\n",
        "\n",
        "    choice = update.message.text\n",
        "    rag_store = user_data[user_id][\"rag\"]\n",
        "\n",
        "    if \"Summarize\" in choice:\n",
        "        if not rag_store.chunks:\n",
        "            await update.message.reply_text(\"âš ï¸ No content loaded yet. Please send a PDF or text first.\")\n",
        "            return\n",
        "        await update.message.reply_text(\"â³ Generating summary...\")\n",
        "        original_text = \" \".join(rag_store.chunks)\n",
        "        summary = await process_with_typing_indicator(\n",
        "          update,\n",
        "          context,\n",
        "          generate_summary,\n",
        "          original_text\n",
        "        )\n",
        "        if summary:\n",
        "            await update.message.reply_text(f\"ðŸ“ Summary:\\n\\n{summary}\\n\\nChoose another option ðŸ‘‡\", reply_markup=MENU_KEYBOARD)\n",
        "\n",
        "    elif \"Explain\" in choice:\n",
        "        if not rag_store.chunks:\n",
        "            await update.message.reply_text(\"âš ï¸ No content loaded yet. Please send a PDF or text first.\")\n",
        "            return\n",
        "        await update.message.reply_text(\"â³ Generating explanation...\")\n",
        "        explanation = await process_with_typing_indicator(update, context, generate_simplified_explanation, \" \".join(rag_store.chunks))\n",
        "        if explanation:\n",
        "            await update.message.reply_text(f\"ðŸ“– Explanation:\\n\\n{explanation}\\n\\nChoose another option ðŸ‘‡\", reply_markup=MENU_KEYBOARD)\n",
        "\n",
        "    elif \"Ask\" in choice:\n",
        "        user_data[user_id][\"mode\"] = \"qa\"\n",
        "        await update.message.reply_text(\n",
        "            \"ðŸ’¬ Ask any question about the content.\\nType 'exit' to return.\",\n",
        "            reply_markup=ReplyKeyboardMarkup([[\"exit\"]], resize_keyboard=True)\n",
        "        )\n",
        "\n",
        "    elif \"Clear\" in choice:\n",
        "        user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
        "        await update.message.reply_text(\"ðŸ§¹ Memory cleared. Send a new document.\", reply_markup=MENU_KEYBOARD)\n",
        "\n",
        "    elif \"Help\" in choice:\n",
        "        await help_command(update, context)\n",
        "\n",
        "async def handle_document(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_id = update.message.from_user.id\n",
        "    if user_id not in user_data:\n",
        "        user_data[user_id] = {\"rag\": RAGStore(), \"mode\": None}\n",
        "\n",
        "    await update.message.reply_text(\"â³ Processing your PDF...\")\n",
        "    try:\n",
        "        file = await update.message.document.get_file()\n",
        "        file_bytes = await file.download_as_bytearray()\n",
        "        text = read_pdf_content(file_bytes)\n",
        "\n",
        "        if not text.strip():\n",
        "            await update.message.reply_text(\"âš ï¸ Could not read text from PDF.\")\n",
        "            return\n",
        "\n",
        "        clean_text_content, _ = clean_text(text)\n",
        "        user_data[user_id][\"rag\"].add_text(clean_text_content, source=\"document\")\n",
        "\n",
        "        await update.message.reply_text(\n",
        "            \"âœ… PDF processed!\\nNow choose: Summarize, Explain, or Ask Questions\",\n",
        "            reply_markup=MENU_KEYBOARD\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"PDF Error: {e}\")\n",
        "        await update.message.reply_text(\"âŒ Failed to process PDF.\")\n",
        "\n",
        "async def handle_text_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:\n",
        "    user_id = update.message.from_user.id\n",
        "    if user_id not in user_data:\n",
        "        await start_command(update, context)\n",
        "        return\n",
        "\n",
        "    text = update.message.text.strip()\n",
        "\n",
        "    if text.lower() in [\"exit\", \"quit\"]:\n",
        "        user_data[user_id][\"mode\"] = None\n",
        "        await update.message.reply_text(\"Back to main menu.\", reply_markup=MENU_KEYBOARD)\n",
        "        return\n",
        "\n",
        "    if user_data[user_id].get(\"mode\") == \"qa\":\n",
        "        rag_store = user_data[user_id][\"rag\"]\n",
        "        if not rag_store.chunks:\n",
        "            await update.message.reply_text(\"âš ï¸ No content to answer questions.\")\n",
        "            return\n",
        "        await update.message.reply_text(\"ðŸ” Finding answer...\")\n",
        "        results = rag_store.search(text, top_k=3)\n",
        "        contexts = [res[0] for res in results]\n",
        "        answer = await process_with_typing_indicator(update, context, generate_answer, text, contexts)\n",
        "        await update.message.reply_text(f\"â“ {text}\\n\\nðŸ’¡ {answer}\\n\\nAsk more or type 'exit'.\")\n",
        "    else:\n",
        "        clean_text_content, _ = clean_text(text)\n",
        "        user_data[user_id][\"rag\"].add_text(clean_text_content, source=\"text\")\n",
        "        await update.message.reply_text(\n",
        "            \"âœ… Text saved!\\nNow choose: Summarize, Explain, or Ask Questions\",\n",
        "            reply_markup=MENU_KEYBOARD\n",
        "        )"
      ],
      "metadata": {
        "id": "0tgEdQ57jRfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bot Application"
      ],
      "metadata": {
        "id": "TIqcegN39fkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_application() -> ApplicationBuilder:\n",
        "    app = ApplicationBuilder().token(\"8326794111:AAHWrN70roC7EK9rJz9LJyjS17ugMNcx5dM\").build()\n",
        "    app.add_handler(CommandHandler(\"start\", start_command))\n",
        "    app.add_handler(CommandHandler(\"help\", help_command))\n",
        "    app.add_handler(MessageHandler(filters.Regex(\"^(Summarize|Explain|Ask|Clear|Help)\"), handle_menu_selection))\n",
        "    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text_message))\n",
        "    app.add_handler(MessageHandler(filters.Document.PDF, handle_document))\n",
        "    return app\n",
        "\n",
        "async def run_bot():\n",
        "    app = setup_application()\n",
        "    await app.run_polling()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    asyncio.run(run_bot())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "6wF4lxLQjWQq",
        "outputId": "495d7b16-16e9-484e-fe86-fb3beebdc733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Cannot close a running event loop",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3381770013.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_future\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     96\u001b[0m                 raise RuntimeError(\n\u001b[1;32m     97\u001b[0m                     'Event loop stopped before Future completed.')\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/futures.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__log_traceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/tasks.py\u001b[0m in \u001b[0;36m__step_run_and_handle_result\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0;31m# We use the `send` method directly, because coroutines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0;31m# don't have `__iter__` and `__next__` methods.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3381770013.py\u001b[0m in \u001b[0;36mrun_bot\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_bot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mapp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_application\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mawait\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_polling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36mrun_polling\u001b[0;34m(self, poll_interval, timeout, bootstrap_retries, read_timeout, write_timeout, connect_timeout, pool_timeout, allowed_updates, drop_pending_updates, close_loop, stop_signals)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m         return self.__run(\n\u001b[0m\u001b[1;32m    695\u001b[0m             updater_coroutine=self.updater.start_polling(\n\u001b[1;32m    696\u001b[0m                 \u001b[0mpoll_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpoll_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/telegram/ext/_application.py\u001b[0m in \u001b[0;36m__run\u001b[0;34m(self, updater_coroutine, stop_signals, close_loop)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mclose_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                     \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoroutine\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCoroutine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/unix_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finalizing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msig\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_signal_handlers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/asyncio/selector_events.py\u001b[0m in \u001b[0;36mclose\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot close a running event loop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Cannot close a running event loop"
          ]
        }
      ]
    }
  ]
}